---
layout: post
title: "[RunPod] GPUê°€ ì—†ì–´ìš”? GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ìš”? ëŸ°íŒŸì„ ì‚¬ìš©í•´ë³´ì„¸ìš” - 2"
subtitle: LLM íŒŒì¸íŠœë‹
categories: í´ë¼ìš°ë“œ
comments: true
use_math: true
---


ì•ˆë…•í•˜ì„¸ìš” jiogenes ì…ë‹ˆë‹¤.

ì˜¤ëŠ˜ì€ ì €ë²ˆ runpod íŠœí† ë¦¬ì–¼ì— ì´ì–´ì„œ runpod ë‚´ì—ì„œ LLMì„ í•™ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤.

LLMì„ í•™ìŠµí•˜ë ¤ë©´ í° GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•œë°ìš”.

ë³´í†µ 7B(70ì–µ)íŒŒë¼ë¯¸í„° ì§œë¦¬ LLMì„ ê·¸ëƒ¥ ë¡œë“œí•œë‹¤ê³  í•˜ë©´ 70ì–µ * 8(byte) = ëŒ€ëµ 56GB ì •ë„ê°€ ë©ë‹ˆë‹¤.

ëª¨ë¸ í›ˆë ¨ì„ í•˜ê³ ì í•œë‹¤ë©´ ê·¸ë ˆë””ì–¸íŠ¸ì™€ optimizerì˜ íŒŒë¼ë¯¸í„°ë¡œ ì¸í•´ ì•½ 3ë°° ë„˜ê²Œ ë©”ëª¨ë¦¬ê°€ ë” í•„ìš”í•˜ë¯€ë¡œ í›ˆë ¨ì„ ìœ„í•´ì„œ ëŒ€ëµ 150GB ì´ìƒì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë‹¤ê³  ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ°ë° ê³ ì‘ 7Bëª¨ë¸ë§Œ í›ˆë ¨í•˜ë ¤ê³  í•´ë„ í˜„ì¡´í•˜ëŠ” ê°€ì¥ í° ìš©ëŸ‰ì˜ ê·¸ë˜í”½ì¹´ë“œì¸ H100, A100(80GB) ì¡°ì°¨ë„ 1ì¥ìœ¼ë¡œëŠ” í„±ì—†ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.

ì´ ê¸€ì—ì„œ ë‹¤ ì„¤ëª…í•  ìˆ˜ ì—†ì§€ë§Œ ì´ëŸ¬í•œ ìš©ëŸ‰ ë¶€ì¡±ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë°©ë²•ì´ ì¡´ì¬í•©ë‹ˆë‹¤.

## ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²° ë°©ë²•

### Mixed precision

ë¨¼ì € ë¶€ë™ì†Œìˆ˜ì ì˜ ì •ë°€ë„ë¥¼ ë‚®ì¶°ì„œ fine-tuningì„ í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ì§§ê²Œ ì„¤ëª…ë“œë¦¬ìë©´ ê¸°ì¡´ ë¶€ë™ì†Œìˆ˜ì  ë°©ì‹ì¸ fp32ëŠ” 32ë¹„íŠ¸ì˜ ë©”ëª¨ë¦¬ë¥¼ í†µí•´ ì†Œìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ë° ì´ë¥¼ fp16ì¸ 16ë¹„íŠ¸ë§Œ ì‚¬ìš©í•´ì„œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë¡œì¸í•´ ì†Œìˆ˜ì ì˜ ìë¦¿ìˆ˜ê°€ ì¤„ê³  ë²”ìœ„ë„ ì¤„ì§€ë§Œ pre-trained modelì—ì„œ inference í˜¹ì€ fine-tuningì„ í•˜ëŠ” ìƒí™©ì´ë¼ë©´ í° ì†ì‹¤ì—†ì´ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì´ ì ˆë°˜ ì¤„ì–´ë“œëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. ì •í™•íˆëŠ” ì ˆë°˜ì´ ì•„ë‹ˆë¼ precisionì„ ë‚®ì¶°ë„ ë˜ëŠ” ì—°ì‚°ë§Œ ë°ì´í„° íƒ€ì…ì„ ë³€ê²½í•˜ê¸° ë•Œë¬¸ì— mixed ë¼ëŠ” í‘œí˜„ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.

í—ˆê¹…í˜ì´ìŠ¤ì—ì„œëŠ” ì´ëŸ¬í•œ mixed precision ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê°„í¸í•œ APIë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.

```python
# ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ë•Œ
model = AutoModel.from_pretrained("model_name", torch_dtype=torch.float16)

# Trainerë¥¼ ì‚¬ìš©í•  ë•Œ
training_args = TrainingArguments(fp16=True, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)
```

ë” ìì„¸í•œ ì„¤ëª…ì„ ë³´ê³ ì‹¶ë‹¤ë©´ [í—ˆê¹…í˜ì´ìŠ¤ ë¬¸ì„œ](https://huggingface.co/docs/transformers/v4.15.0/performance#floating-data-types)ë¥¼ ì½ì–´ë³´ì‹œë©´ ì¢‹ì„ê²ƒ ê°™ìŠµë‹ˆë‹¤.

### PEFT

ê·¸ë¦¬ê³  fine-tuningì‹œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì „ë¶€ fine-tuning í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ íŠœë‹í•˜ëŠ” `PEFT(Parameter Efficient Fine-Tuning)`ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

PEFT ë°©ë²• ì¤‘ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” LoRAì— ëŒ€í•´ ì§§ê²Œ ì„¤ëª…í•˜ê³  ë°”ë¡œ ì‹¤ìŠµí•´ë³´ë„ë¡ í•˜ì£ .

<img width="392" alt="image" src="https://github.com/jiogenes/utterances/assets/43975730/ad355141-982b-41c8-8036-36b901c59b9d">

LoRAëŠ” ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ëª¨ë¸ì˜ ê¸°ì¡´ ì›¨ì´íŠ¸ ì˜†ì— rë§Œí¼ ì°¨ì›ì„ ì¤„ì—¬ì£¼ëŠ” í–‰ë ¬ê³¼ ì›ë˜ í¬ê¸°ë§Œí¼ ë³µêµ¬ì‹œí‚¤ëŠ” í–‰ë ¬ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì§€ë§Œ hidden stateì— ë”í•´ì§€ëŠ” ê°’ìœ¼ë¡œ ì¸í•´ fine-tuningì˜ íš¨ê³¼ëŠ” ìƒë‹¹í•©ë‹ˆë‹¤.

[ì½”ë“œ](https://github.com/microsoft/LoRA/blob/4c0333854cb905966f8cc4e9a74068c1e507c7b7/loralib/layers.py#L90-L152)ë¥¼ ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
class Linear(nn.Linear, LoRALayer):
    # LoRA implemented in a dense layer
    def __init__(
        self,
        in_features: int,
        out_features: int,
        r: int = 0,
        lora_alpha: int = 1,
        lora_dropout: float = 0.,
        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        merge_weights: bool = True,
        **kwargs
    ):
        nn.Linear.__init__(self, in_features, out_features, **kwargs)
        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                           merge_weights=merge_weights)

        self.fan_in_fan_out = fan_in_fan_out
        # Actual trainable parameters
        if r > 0:
            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))
            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))
            self.scaling = self.lora_alpha / self.r
            # Freezing the pre-trained weight matrix
            self.weight.requires_grad = False
        self.reset_parameters()
        if fan_in_fan_out:
            self.weight.data = self.weight.data.transpose(0, 1)

    def reset_parameters(self):
        nn.Linear.reset_parameters(self)
        if hasattr(self, 'lora_A'):
            # initialize B the same way as the default for nn.Linear and A to zero
            # this is different than what is described in the paper but should not affect performance
            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
            nn.init.zeros_(self.lora_B)

    def train(self, mode: bool = True):
        def T(w):
            return w.transpose(0, 1) if self.fan_in_fan_out else w
        nn.Linear.train(self, mode)
        if mode:
            if self.merge_weights and self.merged:
                # Make sure that the weights are not merged
                if self.r > 0:
                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling
                self.merged = False
        else:
            if self.merge_weights and not self.merged:
                # Merge the weights and mark it
                if self.r > 0:
                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling
                self.merged = True

    def forward(self, x: torch.Tensor):
        def T(w):
            return w.transpose(0, 1) if self.fan_in_fan_out else w
        if self.r > 0 and not self.merged:
            result = F.linear(x, T(self.weight), bias=self.bias)
            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling
            return result
        else:
            return F.linear(x, T(self.weight), bias=self.bias)
```

pytorchì˜ nn.Linearë¥¼ ìƒì†ë°›ì•„ Linearì˜ ê¸°ì¡´ í–‰ë™ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìœ¼ë©° ì¶”ê°€ì ì¸ `self.lora_A`ì™€ `self.lora_B`ë¡œ ì¸í•´ ì´ `Linear`í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ì ìœ¼ë¡œ ì›ë˜ íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •ë˜ê³  `self.lora_A`ì™€ `self.lora_B`ë§Œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
íŠ¹íˆ `train`ë©”ì†Œë“œì—ì„œ `model.train()`ê³¼ `model.eval()`(=model.train(False))ì¼ ë•Œ ì›ë˜ ì›¨ì´íŠ¸ì—ì„œ ì¶”ê°€ëœ ì›¨ì´íŠ¸ë¥¼ ë¹¼ê³  í•™ìŠµì„ í•  ê²ƒì¸ì§€ ì›ë˜ ì›¨ì´íŠ¸ì—ì„œ ì¶”ê°€ëœ ì›¨ì´íŠ¸ë¥¼ ë”í•˜ê³  ê·¸ëƒ¥ linear ì—°ì‚°ë§Œ í•  ê²ƒì¸ì§€ êµ¬í˜„í•´ ë†“ì€ ë¶€ë¶„ì„ ë³´ì‹œë©´ ì´í•´ê°€ ë¹ ë¥´ì‹¤ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ìì„¸í•œ ì„¤ëª…ì„ ë³´ê³ ì‹¶ë‹¤ë©´ [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)ì„ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ê²ƒ ê°™ìŠµë‹ˆë‹¤.

## LLM íŒŒì¸íŠœë‹

ìš°ì„  íŒŒì¸íŠœë‹ì„ í•˜ê¸° ìœ„í•´ ì´ë²ˆ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ê³¼ ë°ì´í„° ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- [Llama2](https://huggingface.co/meta-llama/Llama-2-7b)
- [í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°(open korean instructions)](https://huggingface.co/datasets/heegyu/open-korean-instructions)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) (mixed precision)
- [peft](https://github.com/huggingface/peft) (LoRA)

### íŒŸ ìƒì„±

ì €ëŠ” 3090 1ê°œë¡œ í•™ìŠµì„ ì§„í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤. LoRAì™€ Mixed precision ê¸°ìˆ ì— í™ì…ì–´ 3090 1ê°œë¡œë„ Llama2 7B ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ì´ ê°€ëŠ¥í•´ ì¡Œê¸° ë•Œë¬¸ì…ë‹ˆë‹¤!

<img width="911" alt="image" src="https://github.com/jiogenes/utterances/assets/43975730/9a28b4f8-b724-40f7-a714-4253ccbc11df">

ë””ìŠ¤í¬ëŠ” ë°ì´í„°ì™€ ì›¨ì´íŠ¸ íŒŒì¼ì„ ì €ì¥í•  ìˆ˜ ìˆë„ë¡ ë„‰ë„‰í•˜ê²Œ 50GBë¡œ ì¤€ë¹„í•´ ì¤ë‹ˆë‹¤.

<img width="946" alt="image" src="https://github.com/jiogenes/utterances/assets/43975730/fddc2dac-052f-47b9-8fc9-4d87c6df5a8a">

### ê°€ìƒí™˜ê²½ êµ¬ì„± ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

í„°ë¯¸ë„ í˜¹ì€ ì£¼í”¼í„° í„°ë¯¸ë„ì„ ë“¤ì–´ê°€ì„œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— venvë¡œ ê°€ìƒí™˜ê²½ì„ ì„¤ì¹˜í•˜ê³  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ë´…ì‹œë‹¤.

```bash
root@4197544e2316:/workspace# python -m venv llama
root@4197544e2316:/workspace# source llama/bin/activate
(llama) root@4197544e2316:/workspace# pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
(llama) root@4197544e2316:/workspace# pip install trl transformers accelerate peft datasets bitsandbytes wandb ipykernel ipywidgets
```

runpodì—ëŠ” ì´ë¯¸ ê¸°ë³¸ì ìœ¼ë¡œ pytorchê°€ ê¹”ë ¤ìˆê¸° ë•Œë¬¸ì— venv ì—†ì´ ë°”ë¡œ `pip install`ì„ í†µí•´ ì„¤ì¹˜í•´ë„ ìƒê´€ì—†ì§€ë§Œ í”„ë¡œì íŠ¸ë³„ë¡œ ê°€ìƒí™˜ê²½ì„ ë‚˜ëˆ  ê´€ë¦¬í•˜ëŠ” ìŠµê´€ì„ ê°€ì§€ë©´ ì¢‹ìŠµë‹ˆë‹¤.

ì´ì œ venv ê°€ìƒí™˜ê²½ìœ¼ë¡œ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ jupyter kernelì— ê°€ìƒí™˜ê²½ì„ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤.

```bash
(llama) root@4197544e2316:/workspace# pip install ipykernel ipywidgets
(llama) root@4197544e2316:/workspace# deactivate
root@4197544e2316:/workspace# python -m ipykernel install --user --name llama --display-name llama
```

ê·¸ë¦¬ê³  ìƒˆë¡œê³ ì¹¨ F5í‚¤ë¥¼ ëˆŒëŸ¬ì¤€ í›„ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê°€ìƒí™˜ê²½ ì»¤ë„ì˜ ë…¸íŠ¸ë¶ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<img width="694" alt="Untitled 2" src="https://github.com/jiogenes/utterances/assets/43975730/6ad7e7d4-8f03-4421-85a1-83f65db46408">

llama ë…¸íŠ¸ë¶ì„ ë§Œë“¤ê³  ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤.

### í•™ìŠµ ì½”ë“œ

ë¨¼ì € ì‹¤ìŠµì— ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê² ìŠµë‹ˆë‹¤.

```python
!pip install trl transformers accelerate peft datasets bitsandbytes wandb
```

í—ˆê¹…í˜ì´ìŠ¤ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ë³€ê²½í•´ ì¤ì‹œë‹¤. ë””í´íŠ¸ ê²½ë¡œëŠ” ì‚¬ìš©ì í™ˆë””ë ‰í† ë¦¬ ì•ˆì— ìƒì„±ë˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ëŠ˜ë ¤ë†“ì€ Container Diskì— ì €ì¥ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. Container DiskëŠ” /workspaceì˜ ìš©ëŸ‰ë§Œ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import os
cache_dir = '/workspace/cache'

if not os.path.exists(cache_dir):
    os.makedirs(cache_dir)

os.environ['HF_HOME'] = cache_dir
```

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•´ì¤ë‹ˆë‹¤.

```python
from datasets import load_dataset

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

from huggingface_hub import notebook_login

import wandb
```

ì•ì„œ ë§ì”€ë“œë¦°ê²ƒ ì²˜ëŸ¼ llama2 ëª¨ë¸ê³¼ open-korean-instruction ë°ì´í„°ì…‹ì˜ ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì €ì¥í•  ëª¨ë¸ì˜ ì´ë¦„ë„ ì„¤ì •í•´ ë†“ìŠµë‹ˆë‹¤.

```python
model_name = 'meta-llama/Llama-2-7b-hf'
data_name = 'heegyu/open-korean-instructions'
fine_tuning_model_name = f'{model_name}-finetuned-open-korean-instructions'
device_map = 'auto'
```

LoRAì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì•ŒíŒŒê°’ì€ 16, rì€ 64ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. 64ë­í¬ê¹Œì§€ ì••ì¶•í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê² ì£ .

```python
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias='none',
    task_type='CAUSAL_LM'
)
```

Mixed precisionë„ ì„¤ì •í•´ì¤ë‹ˆë‹¤. 4bitë¡œ ë¡œë“œí•˜ê³  ë°˜ë³µí•´ì„œ ì–‘ìí™” í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•´ ì¤ë‹ˆë‹¤.

```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype='float16',
)
```

ë¼ë§ˆ2 ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ í—ˆê¹…í˜ì´ìŠ¤ì— ê°€ì…ì´ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ ìœ„ì ¯ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.

```python
notebook_login()
```

<img width="417" alt="Untitled 3" src="https://github.com/jiogenes/utterances/assets/43975730/5149cf39-593c-4494-8463-6c8ef5519293">

ì•„ì§ í—ˆê¹…í˜ì´ìŠ¤ì— ê°€ì…ì´ ì•ˆë˜ì–´ìˆë‹¤ë©´ í—ˆê¹…í˜ì´ìŠ¤ í™ˆí˜ì´ì§€ì— ë“¤ì–´ê°€ì„œ êµ¬ê¸€ê³„ì •(í˜¹ì€ ë‹¤ë¥¸ ê³„ì •)ìœ¼ë¡œ ê°€ì…ì„ í•©ë‹ˆë‹¤. ê°€ì…í•œ í›„ í˜ì´ì§€ì˜ ì™¼ìª½ íƒ­ì—ì„œ Settings ë¼ëŠ” ë©”ë‰´ë¥¼ í´ë¦­í•´ ë“¤ì–´ê°‘ë‹ˆë‹¤.

<img width="264" alt="Untitled 4" src="https://github.com/jiogenes/utterances/assets/43975730/13f958bc-06a4-48db-afbb-1c39bf9d4a26">

Settingsì— ë“¤ì–´ê°€ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë©”ë‰´ê°€ ë‚˜íƒ€ë‚˜ëŠ”ë°ìš”. ì—¬ê¸°ì„œ Access Tokensë¥¼ í´ë¦­í•©ë‹ˆë‹¤.

<img width="317" alt="Untitled 5" src="https://github.com/jiogenes/utterances/assets/43975730/58955706-0458-444e-b6f0-d73f4356acaa">

New tokenì„ í´ë¦­í•´ì„œ ì´ë¦„ì„ ì ê³  Roleì„ writeë¡œ í•˜ì—¬ ìƒˆë¡œìš´ í† í°ì„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.

<img width="364" alt="Untitled 6" src="https://github.com/jiogenes/utterances/assets/43975730/7095fae6-c311-4989-a490-e30e70e3b5a3">

ìƒì„±ëœ í† í°ì„ ë³µì‚¬í•´ì„œ ì•„ê¹Œ ë§Œë“¤ì–´ì§„ ì…ë ¥ì°½ì— ë³µì‚¬í•´ì¤ì‹œë‹¤.

<img width="562" alt="Untitled 7" src="https://github.com/jiogenes/utterances/assets/43975730/d15968dc-1af0-4b54-811e-0742f125d1a7">

ê·¸ëŸ¬ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìœ„ì ¯ì´ ë°”ë€Œë©´ì„œ ë¡œê·¸ì¸ì´ ë©ë‹ˆë‹¤.

<img width="415" alt="Untitled 8" src="https://github.com/jiogenes/utterances/assets/43975730/e8c4906e-9416-44e7-974a-e2ad24c8b78c">

í˜¹ì‹œ ë¼ë§ˆ2 ëª¨ë¸ì„ ì‚¬ìš©í•´ë³¸ ê²½í—˜ì´ ì—†ë‹¤ë©´ [í—ˆê¹…í˜ì´ìŠ¤ ë¼ë§ˆ2 ëª¨ë¸ í™ˆí˜ì´ì§€](https://huggingface.co/meta-llama/Llama-2-7b-hf)ë¥¼ ë“¤ì–´ê°€ì„œ ëª¨ë¸ ì ‘ê·¼ì— ëŒ€í•œ ë™ì˜ë¥¼ ì–»ì–´ì•¼ í•©ë‹ˆë‹¤.

<img width="733" alt="Untitled 9" src="https://github.com/jiogenes/utterances/assets/43975730/afbbfbb5-82ea-4cbd-957e-96e76ca8e188">

ìœ„ ê·¸ë¦¼ ì—ì„œ ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ ë°”ë€Œë©´ ë©ë‹ˆë‹¤.

<img width="486" alt="Untitled 10" src="https://github.com/jiogenes/utterances/assets/43975730/c3882cab-7a5f-41ce-bd74-724f6fc0723b">

ë‹¤ì‹œ ì½”ë“œë¡œ ëŒì•„ì™€ì„œ ìš°ë¦¬ê°€ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ wandbì— ë¡œê·¸ì¸ í•©ë‹ˆë‹¤. wandbì—­ì‹œ [í™ˆí˜ì´ì§€](https://wandb.ai/site)ì—ì„œ ê°€ì…ì´ í•„ìš”í•©ë‹ˆë‹¤.

```python
wandb.login()
wandb.init(project=fine_tuning_model_name.split('/')[-1])
```

<img width="851" alt="Untitled 11" src="https://github.com/jiogenes/utterances/assets/43975730/e409d88a-e9a6-4328-8e3f-56af26b08ec9">

ê°€ì… í›„ [ë‹¤ìŒ ë§í¬](https://wandb.ai/authorize)ì—ì„œ ì—‘ì„¸ìŠ¤ í† í°ì„ ì–»ì–´ ì…ë ¥ì°½ì— ë¶™ì—¬ë„£ê¸° í•´ ì¤ë‹ˆë‹¤.

<img width="411" alt="Untitled 12" src="https://github.com/jiogenes/utterances/assets/43975730/28167e27-5a76-45de-8a18-9a538d65376d">

ì ì´ì œ ê·€ì°®ì€ ì‘ì—…ë“¤ì´ ëª¨ë‘ ì™„ë£ŒëìŠµë‹ˆë‹¤. ë³¸ê²©ì ì¸ í•™ìŠµ ì½”ë“œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

ë¨¼ì €, ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  ë°ì´í„°ë¥¼ í•œë²ˆ ì°ì–´ë³´ê² ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ ë§ì„ìˆ˜ë¡ ë‹¹ì—°íˆ ì¢‹ê² ì§€ë§Œ í•™ìŠµ ì‹œê°„ì´ ë” ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ë°ì´í„°ì…‹ì˜ 10%ë§Œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.

```python
dataset = load_dataset(data_name, split='train[:10%]')
print(dataset[0]['text'])

>>>
<usr> ìœ ì–¸ì¥ì´ ìˆëŠ” ê²ƒì´ ì¢‹ë‹¤ëŠ” ë§ì„ ë“¤ì—ˆìŠµë‹ˆë‹¤. ìœ ì–¸ì¥ì´ë€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
<bot> ìœ ì–¸ì¥ì€ ê·€í•˜ê°€ ì‚¬ë§í•œ í›„ ê·€í•˜ì˜ ì¬ì‚°ì´ ì–´ë–»ê²Œ ë¶„ë°°ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì§€ì •í•˜ëŠ” ë²•ì  ë¬¸ì„œì…ë‹ˆë‹¤. ë˜í•œ ê·€í•˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìë…€ë‚˜ ê¸°íƒ€ ë¶€ì–‘ê°€ì¡±ì„ ëˆ„ê°€ ëŒë´ì•¼ í•˜ëŠ”ì§€ ëª…ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ ì–¸ì¥ì— ì ìš©ë˜ëŠ” ë²•ë¥ ì´ ì£¼ë§ˆë‹¤ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ê·€í•˜ì˜ ìœ ì–¸ì¥ì´ ìœ íš¨í•˜ê³  ìµœì‹ ì¸ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
```

<usr> ê³¼ <bot> ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ì±—ë´‡ ëŒ€ë‹µì´ ë‚˜ë‰˜ì–´ ì§€ëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê² ìŠµë‹ˆë‹¤. ë¼ë§ˆ2 ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  peftì™€ mixed precisionì„ ì ìš©í•œ ëª¨ë¸ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.

```python
base_model = AutoModelForCausalLM.from_pretrained(model_name,
                                             quantization_config=bnb_config,
                                             use_cache=False,
                                             device_map=device_map)
base_model.config.pretraining_tp = 1
base_model.gradient_checkpointing_enable()
base_model = prepare_model_for_kbit_training(base_model)
peft_model = get_peft_model(base_model, peft_config)
```

í† í¬ë‚˜ì´ì €ë„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

```python
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
```

í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ ì¤ì‹œë‹¤. ì—¬ê¸°ì„œ íŠ¹íˆ `per_device_train_batch_size`ëŠ” ì´ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ë‚˜íƒ€ë‚´ê³  `gradient_accumulation_steps`ëŠ” ì›í•˜ëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ í•œë²ˆì— ë„£ì„ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ ë‚˜ëˆ ì„œ ë„£ê³  ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•´ì„œ ë§ˆì§€ë§‰ì— ìµœì í™” í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.

```python
training_args = TrainingArguments(
    output_dir=fine_tuning_model_name,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    gradient_checkpointing=True,
    optim='paged_adamw_32bit',
    logging_steps=5,
    save_strategy='epoch',
    learning_rate=2e-4,
    weight_decay=0.001,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    group_by_length=False,
    lr_scheduler_type='cosine',
    disable_tqdm=True,
    report_to='wandb',
    seed=42
)
```

í•™ìŠµì€ ê¸°ë³¸ íŠ¸ë ˆì´ë„ˆë¡œë„ ì˜ ë™ì‘í•˜ì§€ë§Œ SFTTrainerë¥¼ ì‚¬ìš©í•˜ë©´ ë” íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://huggingface.co/docs/trl/sft_trainer)ë¥¼ ì°¸ì¡°í•´ ì£¼ì„¸ìš”.

```python
trainer = SFTTrainer(
    model=peft_model,
    train_dataset=dataset,
    dataset_text_field='text',
    max_seq_length=min(tokenizer.model_max_length, 2048),
    tokenizer=tokenizer,
    packing=True,
    args=training_args
)
```

ì´ì œ í•™ìŠµì„ í•´ë´…ì‹œë‹¤.

```python
trainer.train()
```

### ëª¨ë¸ ì €ì¥ ë° í‰ê°€

í•™ìŠµ ì¤‘ì—ëŠ” ì´ë ‡ê²Œ ì™„ë””ë¹„ì—ì„œ í•™ìŠµë¥ ê³¼ ë¡œìŠ¤, í•˜ë“œì›¨ì–´ ì‚¬ìš©ëŸ‰ ë“±ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<img width="907" alt="Untitled 13" src="https://github.com/jiogenes/utterances/assets/43975730/24187a32-6089-435e-9d50-8ad368f77d1c">

í•™ìŠµì´ ëë‚˜ë©´ ì™„ë””ë¹„ë¥¼ ì¢…ë£Œí•˜ê³  ëª¨ë¸ì„ ì €ì¥í•´ ì¤ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ LoRAì˜ ì—…ë°ì´íŠ¸ëœ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥í•©ë‹ˆë‹¤.

```python
wandb.finish()
trainer.save_model()
```

í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ merge_and_unload ë©”ì†Œë“œë¥¼ í†µí•´ LoRAë¡œ ì—…ë°ì´íŠ¸ ëœ ì›¨ì´íŠ¸ë²¡í„°ë¥¼ ì›ë˜ ì›¨ì´íŠ¸ì— ë”í•´ì¤ë‹ˆë‹¤. ë”í•´ì§„ ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•´ ì¤ë‹ˆë‹¤.

```python
trained_model = AutoPeftModelForCausalLM.from_pretrained(
    training_args.output_dir,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map
)

lora_merged_model = trained_model.merge_and_unload()
lora_merged_model.save_pretrained('merged', safe_serialization=True)
tokenizer.save_pretrained('merged')
```

ì €ì¥ëœ ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.

```python
lora_merged_model.push_to_hub(training_args.output_dir)
tokenizer.push_to_hub(training_args.output_dir)
```

ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•´ ë´…ë‹ˆë‹¤.

```python
prompt = '<usr> íŠ¸ëœìŠ¤í¬ë¨¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\n<bot> '
input_ids = tokenizer(prompt, return_tensors='pt', truncation=True).input_ids.cuda()

print(f"-------------------------\n")
print(f"Prompt:\n{prompt}\n")
print(f"-------------------------\n")

print(f"Base Model Response :\n")
output_base = model.generate(input_ids=input_ids, max_new_tokens=500, do_sample=True, top_p=0.9,temperature=0.5)
print(f"{tokenizer.batch_decode(output_base.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}")
print(f"-------------------------\n")

print(f"Trained Model Response :\n")
trained_model = lora_merged_model.generate(input_ids=input_ids, max_new_tokens=500, do_sample=True, top_p=0.9,temperature=0.5)
print(f"{tokenizer.batch_decode(trained_model.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}")
print(f"-------------------------\n")

print(f"LORA Model Response :\n")
output_trained_lora = lora_merged_model.generate(input_ids=input_ids, max_new_tokens=500, do_sample=True, top_p=0.9,temperature=0.5)
print(f"{tokenizer.batch_decode(output_trained_lora.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}")
print(f"-------------------------\n")
```

ëŸ°íŒŸì„ ì´ìš©í•´ì„œ LLMì„ í•™ìŠµí•˜ëŠ” ì‹¤ìŠµì„ ì§„í–‰í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. ì €ëŠ” ì•½ 5ì‹œê°„ ì •ë„ê°€ ê±¸ë ¸ê³  ì‹œê°„ë‹¹ 0.29ë‹¬ëŸ¬ë¡œ ì•½ 2ë‹¬ëŸ¬ê°€ ì•ˆë˜ëŠ” ëˆìœ¼ë¡œ Llama2 ëª¨ë¸ì„ íŒŒì¸íŠœë‹ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ë°ì´í„° ì „ì²˜ë¦¬ì™€ ë‹¤ì–‘í•œ í•™ìŠµ ë°©ë²•ë“¤ì„ ì ìš©í•œë‹¤ë©´ ì´ê²ƒë³´ë‹¤ ë” ì¢‹ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ê²ƒì…ë‹ˆë‹¤. ì¢‹ì€ LLMì€ ë‹¨ì§€ ë°ì´í„°ë§Œ ë§ì´ ë„£ê³  ëŒë¦¬ëŠ” ìˆ˜ì¤€ì—ì„œ ë§Œë“¤ì–´ ì§€ëŠ”ê²Œ ì•„ë‹ˆë¼ ìˆ˜ë§ì€ ì‚½ì§ˆ(ë°ì´í„° ì „ì²˜ë¦¬)ì— ì‚½ì§ˆ(ì•„í‚¤í…ì³ ìˆ˜ì •)ì„ ê±°ë“­í•´ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¬¼ì¼ ê²ƒì…ë‹ˆë‹¤.

GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ LLMì„ ê±´ë“œë ¤ë³¼ ì‹œë„ì¡°ì°¨ ëª»í•´ë³¸ ì‚¬ëŒë„ ë§ê² ì§€ë§Œ ìš°ë¦¬ëŠ” ëŸ°íŒŸì„ ì´ìš©í•´ì„œ (ëˆì´ ì¡°ê¸ˆ ë“¤ê¸´ í•˜ê² ì§€ë§Œ) ì‚½ì§ˆì„ í•´ë³¼ ìˆ˜ ìˆëŠ” **ê¸°íšŒ**ë¥¼ ì–»ì€ ê²ƒì…ë‹ˆë‹¤. ì´ ê¸°íšŒë¥¼ ì˜ í™œìš©í•´ì„œ LLMì—°êµ¬ê°€ ë” í™œë°œíˆ ì§„í–‰ëìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.

ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ ğŸ¤—

## ì°¸ê³ ìë£Œ

- https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f
- https://huggingface.co/datasets/heegyu/open-korean-instructions
- https://huggingface.co/meta-llama/Llama-2-7b-hf
- https://github.com/huggingface/peft
- https://huggingface.co/docs/diffusers/main/en/training/lora
- https://github.com/microsoft/LoRA
- https://arxiv.org/abs/2106.09685
- https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one
- https://huggingface.co/docs/trl/v0.7.4/en/sft_trainer#trl.SFTTrainer
- https://huggingface.co/blog/4bit-transformers-bitsandbytes
