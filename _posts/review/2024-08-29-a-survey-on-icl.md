---
layout: post
title: "[ë…¼ë¬¸ë¦¬ë·°] A Survey on In-context Learning"
subtitle:
categories: ë…¼ë¬¸ë¦¬ë·°
comments: true
use_math: true
---

ì•ˆë…•í•˜ì„¸ìš” jiogenesì…ë‹ˆë‹¤.

ì˜¤ëŠ˜ì€ NLPì—ì„œ ëœ¨ê±°ìš´ ì£¼ì œì¸ In-context learningì— ëŒ€í•œ [ì„œë² ì´ ë…¼ë¬¸](#ref1)ì„ ë¦¬ë·°í•´ë³´ê³ ì í•©ë‹ˆë‹¤.

ìœ„ ë…¼ë¬¸ì€ In-context learning(ICL)ì— ëŒ€í•œ ì •ì˜ì™€ ICLì„ ì ìš©í•˜ê¸° ìœ„í•œ ìµœì‹  ê¸°ìˆ ë“¤ì— ëŒ€í•œ ì†Œê°œ ë° ICLì„ í™œìš©í•œ applicationì— ëŒ€í•´ ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì´ ê¸€ì—ì„œëŠ” ì œê°€ ì½ì€ ë…¼ë¬¸ì— ëŒ€í•´ì„œ ê°„ëµí•˜ê²Œ ì†Œê°œí•´ ë†“ì•˜ìœ¼ë©°, ì‹¤ì œë¡œëŠ” ë” ë§ì€ ë‚´ìš©ì´ ìˆìœ¼ë¯€ë¡œ ë” ìì„¸íˆ ì•Œê³ ì‹¶ì€ ë¶€ë¶„ì€ ì‹¤ì œ ë…¼ë¬¸ì˜ ì„¹ì…˜ì—ì„œ ì°¸ê³  ë…¼ë¬¸ì„ í†µí•´ ì•Œì•„ë³´ì‹œë©´ ì¢‹ì„ê²ƒ ê°™ìŠµë‹ˆë‹¤.

## Introduction

LLMì˜ ê·œëª¨ê°€ ì»¤ì§€ë©´ì„œ LLMì€ ICL(In-Context Learning) ëŠ¥ë ¥ì„ ê°–ì¶”ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ICLì´ë€ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì˜ˆì‹œ(demonstrations)ë¥¼ ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ì…ë ¥í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì œì˜ ì •ë‹µì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë§í•©ë‹ˆë‹¤. ICLì€ ì§€ë„ í•™ìŠµê³¼ ë‹¬ë¦¬ ë³„ë„ì˜ í•™ìŠµ ê³¼ì •ì´ í•„ìš” ì—†ê³ , ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ê·¸ë ˆë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ë„ ìš”êµ¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ ë†€ë¼ìš´ ICL ëŠ¥ë ¥ìœ¼ë¡œ ì¸í•´ ê±°ì˜ ëª¨ë“  NLP ë¬¸ì œì— LLMì´ ì‚¬ìš©ë¨ì€ ë¬¼ë¡ , ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ì˜ í…ŒìŠ¤í¬ì—ë„ LLMì„ í™œìš©í•˜ê³ ì í•˜ëŠ” ì‹œë„ê°€ ëŠ˜ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.

> ğŸ’¡ NLPì—ì„œ í”„ë¡¬í”„íŠ¸(prompt)ëŠ” ì£¼ë¡œ LLMì˜ ì…ë ¥ì— ë“¤ì–´ê°€ëŠ” ìì—°ì–´ í† í°ë“¤ì„ ì˜ë¯¸í•˜ë©° demonstration(ì˜ˆì‹œ, ì‹œë²”)ì€ few-shot ì˜ˆì œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ë³¸ ë…¼ë¬¸ì€ ICLì˜ ì£¼ìš” ì´ì ì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•©ë‹ˆë‹¤:

1. **í•´ì„ ê°€ëŠ¥í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ**: Demonstrationì´ ìì—°ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´, í”„ë¡¬í”„íŠ¸ì™€ í…œí”Œë¦¿ì„ ìˆ˜ì •í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ LLMì— ì§€ì‹ì„ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ì‚¬ëŒê³¼ ìœ ì‚¬í•œ ì˜ì‚¬ê²°ì •**: ì¸ê°„ì˜ ì‚¬ê³ ë°©ì‹ê³¼ ê°™ì€ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ë”°ë¦…ë‹ˆë‹¤.
3. **í•™ìŠµ ì—†ëŠ” í”„ë ˆì„ì›Œí¬**: ë³„ë„ì˜ í•™ìŠµ ì—†ì´ë„ ì ìš© ê°€ëŠ¥í•´ LLM-as-a-serviceê°€ ê°€ëŠ¥í•˜ë©° ì‹¤ì„¸ê³„ applicationì— ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ICLì˜ ë§ì€ ì¥ì ì´ ìˆì§€ë§Œ, ICLì€ ì—¬ì „íˆ ì—°êµ¬ê°€ì¹˜ê°€ ìƒë‹¹í•©ë‹ˆë‹¤.

1. ì‚¬ì „ í•™ìŠµ(pretraining) ë°©ì‹ì— ë”°ë¼ ICLì˜ ì„±ëŠ¥ì´ í¬ê²Œ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤.
2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, ì˜ˆì‹œ ì„ íƒ ë° ìˆœì„œ(demonstration selection, order of demonstration examples)ì™€ ê°™ì€ settingì— ë§ì€ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤.
3. í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ê³„ì‚° ë¹„ìš©ê³¼ íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ë¥¼ ìœ„í•œ ì—°êµ¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.
4. ICLì˜ ì‘ë™ ì›ë¦¬ê°€ ëª…í™•í•˜ì§€ ì•Šì•„ ì´ë¥¼ í•´ì„í•˜ë ¤ëŠ” ì—°êµ¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.

ìœ„ ì‚¬í•­ë“¤ì„ ê°„ëµí•˜ê²Œ ì•Œì•„ë³´ë„ë¡ í•˜ì£ .

![Untitled](/assets/images/in_posts/a-survey-on-icl/Untitled.png)

## Definition and Formulation

ë…¼ë¬¸ì—ì„œëŠ” ì¸í’‹ í…ìŠ¤íŠ¸ $x$ ì™€ answerì˜ í›„ë³´ë“¤ì¸ $Y=\{y_1, â€¦, y_m\}$ì´ ì£¼ì–´ì¡Œì„ë•Œ , pretrain model $\mathcal{M}$ì€ xì™€ í•¨ê»˜ demonstration set $\mathcal{C}$ì˜ ì˜ˆì‹œë“¤ì„ ì„ íƒí•˜ì—¬ ê°€ì¥ ì¢‹ì€ ì ìˆ˜ë¥¼ ì–»ëŠ” $y$ë¥¼ ì„ íƒí•œë‹¤ê³  ì •ì˜í•©ë‹ˆë‹¤. ì´ ë•Œ,  $\mathcal{C} = \\{ I, s(x_1, y_1), \ldots, s(x_k, y_k) \\} \quad$ or $\quad \mathcal{C} = \\{ s'(x_1, y_1, I), \ldots, s'(x_k, y_k, I) \\}$ ì´ë©°, ì—¬ê¸°ì„œ $I$ëŠ” optional task instruction(ë“¤ì–´ê°ˆ ìˆ˜ë„ ìˆê³  ì•ˆë“¤ì–´ê°ˆ ìˆ˜ë„ ìˆëŠ” í…ŒìŠ¤í¬ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•œ ë¬¸ì¥ì„ ì˜ë¯¸í•˜ëŠ” ë“¯ í•¨), $k$ëŠ” demonstration ê°¯ìˆ˜ ê·¸ë¦¬ê³  $s(\cdot)$ëŠ” í…œí”Œë¦¿ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

> ğŸ’¡ $I, s$ì— ëŒ€í•œ ì„¤ëª…ì´ ì•ˆë‚˜ì™€ ìˆì–´ ì£¼ê´€ì  í•´ì„ì´ ë“¤ì–´ê°”ìŠµë‹ˆë‹¤.

ì´ì œ, answer í›„ë³´ $y_j$ì˜ likelihoodëŠ” ìŠ¤ì½”ì–´ í‘ì…˜ì¸ $f$ë¡œ ë¶€í„° ê³„ì‚°ë˜ì–´ì§‘ë‹ˆë‹¤:

$$
P(y_j | x) \triangleq f_{\mathcal{M}}(y_j, \mathcal{C}, x)
$$

ê·¸ë¦¬ê³  ìµœì¢…ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ëŠ” label $\hat{y}$ëŠ” ê³„ì‚°ëœ í›„ë³´ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì„ ê°€ì§€ëŠ” í›„ë³´ì…ë‹ˆë‹¤:

$$
\hat{y} = \arg\max_{y_j \in Y} P(y_j | x)
$$

ë˜í•œ ë…¼ë¬¸ì—ì„œëŠ” ICLê³¼ ë¹„êµí•˜ì—¬ prompt learning, few-shot learningê³¼ì˜ ì°¨ì´ì ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. prompt learningì€ promptë¡œ ë“¤ì–´ê°€ëŠ” í† í°ì„ ì¡°ì‘í•˜ëŠ” ëª¨ë“  ë°©ë²•ì„ ì¹­í•˜ë©° ì´ëŠ” í† í° ì„ë² ë”©ì„ ì ì ˆíˆ ì¡°ì ˆí•˜ê±°ë‚˜ í•™ìŠµí•˜ëŠ” ë°©ë²•ë„ í¬í•¨ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” ICLì´ prompt learningì˜ í•˜ìœ„ ë¶„ë¥˜ë¼ê³  ë´…ë‹ˆë‹¤. ë˜í•œ, few-shot learningì€ ì „í†µì ì¸ machine learningì—ì„œ ë¼ë²¨ë§ ë°ì´í„°ê°€ ë¶€ì¡±í•  ê²½ìš° ì‚¬ìš©ë˜ë˜ ë°©ë²•ìœ¼ë¡œ í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ” ICLê³¼ ì°¨ì´ê°€ ìˆë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤.

## Model Training

ICL ìì²´ëŠ” í•™ìŠµì´ í•„ìš”ì—†ì§€ë§Œ ICLëŠ¥ë ¥ì„ ë” ê°•í™”í•˜ê¸° ìœ„í•´ pretrain model $\mathcal{M}$ì„ íŠ¹ìˆ˜í•œ ë°©ë²•ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

![Untitled](/assets/images/in_posts/a-survey-on-icl/2974c49f-adcf-4fcc-95e3-5ec0a483341c.png)

### Pretraining

ê°€ì¥ ì§ê´€ì ì¸ ë°©ë²•ìœ¼ë¡œ ICL ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° ì§ì ‘ í•™ìŠµí•˜ê±°ë‚˜ ì¢‹ì€ demonstrationì„ ë§Œë“¤ê¸° ìœ„í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

![Untitled](/assets/images/in_posts/a-survey-on-icl/e8124f61-7a9c-4f00-a04a-36418b5e6708.png)

[In-context pretraining](#ref2) ë°©ë²•ì€ ê¸´ contextë¥¼ í•™ìŠµí•  ë•Œ ì„œë¡œ ì—°ê´€ì´ ì—†ëŠ” ë¬¸ì„œë“¤ì„ ì´ì–´ë¶™ì—¬ì„œ auto-aggressive í•˜ê²Œ í•™ìŠµí•˜ëŠ” ê¸°ì¡´ ë°©ë²•ì„ ê°œì„ í•˜ì—¬, ì„œë¡œ ì—°ê´€ìˆëŠ” ë¬¸ì„œë“¤ì„ DBì—ì„œ ê²€ìƒ‰í•´ ì´ì–´ë¶™ì—¬ ê¸´ contextë¥¼ ë§Œë“¤ê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. Long contextì— ê´€í•œ ì €ìë“¤ì˜ [instight](#ref3)ì™€ ì—°ê´€ìˆëŠ” ë¬¸ì„œë“¤ì„ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ëˆˆì—¬ê²¨ë´ì•¼í•  ë¶€ë¶„ì…ë‹ˆë‹¤.

![Untitled](/assets/images/in_posts/a-survey-on-icl/d05da903-fc67-4ad8-b46e-5f81507fa56c.png)

[MEND](#ref4)ëŠ” ê¸´ contextë¡œ ì¸í•œ memory costë¥¼ ì¤„ì´ë©´ì„œ ICL ì„±ëŠ¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì¼ì¢…ì˜ ICL efficiency ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ 2ê°œì˜ LLMì„ í†µí•´ demonstrationì˜ ì •ì œ(ì••ì¶•) ì„±ëŠ¥ì„ ìœ„í•œ lossë¥¼ ê³„ì‚°í•˜ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì›ë˜ ì„±ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•œ lossë¥¼ ê³„ì‚°í•˜ì—¬ MENDë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ MENDëŠ” ê¸´ demonstrationsë¥¼ ì…ë ¥ë°›ì•„ distilled demonstration vectorë¥¼ ìƒì„±í•˜ê³ , ì´ ë²¡í„°ë¥¼ LLMì˜ ì„ë² ë”© ë²¡í„° ì•ì— ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ ì—†ì´ LLMì˜ context window sizeë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Warmup

LLMì˜ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€ í•™ìŠµì„ í†µí•´ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. LLMì„ fine-tuningí•˜ëŠ” ë°©ë²•ì˜ ì¼ì¢…ì¸ PEFTì˜ ë°©ë²•ì˜ ICL ë²„ì „ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤.

![Untitled](/assets/images/in_posts/a-survey-on-icl/8e1f8c4e-d531-4f46-a42c-c663600a6821.png)

[MetaICL](#ref5)ì€ ICLì„ í†µí•´ ë‹¤ì–‘í•œ í…ŒìŠ¤í¬ë¥¼ í’€ê¸°ìœ„í•œ ìƒˆë¡œìš´ ë©”íƒ€í•™ìŠµ ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ k+1ê°œì˜ exampleì„ ë½‘ì•„ kê°œì˜ exampleì„ í†µí•´ demonstrationì„ êµ¬ì„±í•˜ê³  ë‚¨ì€ 1ê°œë¡œ ë©”íƒ€ í•™ìŠµì„ ì§„í–‰í•¨ìœ¼ë¡œì¨ ì ì€ ìˆ˜ì˜ ì˜ˆì œë§Œìœ¼ë¡œ multi-taskì— ëŒ€í•œ inference ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìœ¼ë©°, ê° í…ŒìŠ¤í¬ì˜ ë°ì´í„°ì…‹ì„ ë³€í˜•í•˜ì§€ ì•Šê³  í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

![image.png](/assets/images/in_posts/a-survey-on-icl/image.png)

[ë˜ ë‹¤ë¥¸ ì—°êµ¬(Chen et al.)](#ref6)ëŠ” self-supervised learningì„ pre-trainê³¼ downstream few-shot evaluation ì‚¬ì´ì—ì„œ ì§„í–‰í•¨ìœ¼ë¡œì¨ ICL ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. Self-supervised learningì€ BERT, GPT, FLAN ë“±ì—ì„œ ì‚¬ìš©í•œ ì—¬ëŸ¬ left-to-right language modeling í•™ìŠµë°©ë²•ì…ë‹ˆë‹¤.

## Prompt Designing

### Demonstration Organization

ë§ì€ ì—°êµ¬ë“¤ì´ demonstationì˜ selection, formatting, orderingì— ë”°ë¼ ICLì˜ ì„±ëŠ¥ì— í¬ê²Œ ì˜í–¥ì„ ì¤€ë‹¤ê³  í•©ë‹ˆë‹¤. ì´ ì„¸ ë¶€ë¶„ì— ê°ê° ì„¸ë¶€ì ìœ¼ë¡œ ì‚´í´ë³´ë„ë¡ í•˜ì£ .

#### Demonstation Selection

Demonstration selectionì€ ICL ì„±ëŠ¥ì„ ìœ„í•´ **â€œì–´ë–¤ ìƒ˜í”Œì´ ê°€ì¥ ì¢‹ì€ ì˜ˆì‹œì¼ê¹Œ?â€** ë¼ëŠ” ë¬¼ìŒì—ì„œ ì¶œë°œí•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” demonstration selection ë¬¸ì œë¥¼ **ë¹„ì§€ë„ í•™ìŠµ ë°©ë²•(Unsupervised Method)**ê³¼ **ì§€ë„ í•™ìŠµ ë°©ë²•(Supervised Method)**ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ìœ„ ì—ì„œ ë´¤ë˜ê²ƒ ì²˜ëŸ¼ modelì„ í•™ìŠµí•˜ëŠ”ì§€ ì•ˆí•˜ëŠ”ì§€ì— ëŒ€í•œ ê²ƒì´ ì•„ë‹ˆë¼, demonstrationì„ ë½‘ëŠ” retriever(ê²€ìƒ‰ê¸°)ë¥¼ ë¹„ì§€ë„ í•™ìŠµê³¼ ì§€ë„ í•™ìŠµìœ¼ë¡œ êµ¬ë¶„í•´ ì„¤ëª…í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë…¼ë¬¸ê³¼ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ **unsupervised retriever**, **supervised retriever**ë¡œ ëª…ëª…í•´ì„œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

**Unsupervised retriever**ëŠ” ì§ê´€ì ìœ¼ë¡œ input instanceì™€ ê°€ì¥ ìœ ì‚¬í•œ ì˜ˆì‹œë“¤ì„ ì„ íƒí•˜ì—¬ demonstrationì„ ë§Œë“œëŠ” retrieverë¥¼ ëœ»í•©ë‹ˆë‹¤. ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ê³„ì‚°í•˜ê¸° ìœ„í•´ L2 distanceì™€ cosine similarityì™€ ê°™ì€ distrance metricì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” machine learningì—ì„œ ëŒ€í‘œì ì¸ ë¹„ì§€ë„í•™ìŠµ ë°©ë²• kNN, ê·¸ë˜í”„ì™€ confidence score, mutual information, perplexity ë“±ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²• ë“±ì„ ì†Œê°œí•˜ê³  ìˆìœ¼ë©°, LLMì˜ outputì„ ë¹„ì§€ë„ í•™ìŠµì˜ í‰ê°€ì§€í‘œë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ë„ ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì €ëŠ” ëŒ€í‘œì ì¸ 2ê°€ì§€ë§Œ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.

![Untitled](/assets/images/in_posts/a-survey-on-icl/1d01f3b1-8669-49fc-8d07-60effa587bed.png)

ë¨¼ì €, [KATE](#ref7)ì€ RoBERTaì™€ sentence embedding modeuleì„ í†µí•´ query inputê³¼ ì–´íœ˜ì , ì˜ë¯¸ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ë¬¸ì¥ì„ ì°¾ì•„ demonstrationì„ êµ¬ì„±í•˜ëŠ” exampleë“¤ì„ selectioní•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ randomí•˜ê²Œ sampleì„ ë½‘ëŠ”ê²ƒ ë³´ë‹¤ ë” ì¢‹ê³  ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

![image.png](/assets/images/in_posts/a-survey-on-icl/0d000e4d-f26a-411c-90d5-2238538cbe9b.png)


[Mutual Informationì„ ì‚¬ìš©í•˜ëŠ” ì—°êµ¬](#ref8)ëŠ” ëª¨ë¸íŒŒë¼ë¯¸í„° ì ‘ê·¼ê³¼ labeled examples ì—†ì´ë„ demonstrationì„ ì ìš©í•˜ëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. Mutual Information $I$ëŠ” ì—”íŠ¸ë¡œí”¼ $H$ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆê³  $I(X;Y)=H(Y)-H(Y|X)$, ê°ê°ì˜ exampleê³¼ templateì˜ MIê³„ì‚°ì„ í†µí•´ ì—¬ëŸ¬ prompt template $\theta$
ì¤‘ì—ì„œ ê°€ì¥ ë‹¤ì–‘í•œ ì‘ë‹µì„ ê³ ë ¤í•¨ê³¼ ë™ì‹œì— ê°€ì¥ confidenceê°€ ë†’ì€
$\theta$ë¥¼ ê³ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ [Self-Adaptive In-Context Learning ë°©ë²•](#ref10)ì€ demonstration selectionê³¼ orderingì„ ë™ì‹œì— ê³ ë ¤í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. (Demonstration Orderingì€ ë’¤ì— ë‚˜ì˜¤ì§€ë§Œ unsupervised retrieverë¼ëŠ” ê´€ì ì—ì„œ ë´ì£¼ì‹œë©´ ë ê²ƒê°™ìŠµë‹ˆë‹¤) Demonstration selection & orderingì„ ì¼ì¢…ì˜ search problemìœ¼ë¡œ ë³´ê³ , top-Kê°œì˜ exampleì„ ë½‘ì•„ search spaceë¥¼ ì¤„ì¸ ë’¤ì— Minimum Description Length (MDL) ì›ë¦¬ë¥¼ ì´ìš©í•´ input promptê°€ ì •ë‹µ yë¡œ ê°€ì¥ ì˜ ì••ì¶•ë  ìˆ˜ ìˆëŠ” demonstration orderë¥¼ ì„ íƒí•©ë‹ˆë‹¤. top-KëŠ” [KATE](#ref7)ì—ì„œ ì‚¬ìš©ëœ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

**Supervised retriever**ëŠ” íŠ¹ì • taskì— íœ´ë¦¬ìŠ¤í‹±í•˜ê³  sub-optimalí•œ unsupervised retrieverì™€ ë‹¬ë¦¬, íŠ¹ì • taskë¥¼ ì¢€ ë” ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµì„ í•˜ëŠ” retrieverì…ë‹ˆë‹¤.

![image.png](/assets/images/in_posts/a-survey-on-icl/0a65ea8c-8b56-45f1-b1d1-8c7e9bb04e26.png)

[EPR](#ref9)ì€ two-stage ë°©ì‹ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ”ë°, ë¨¼ì € unsupervised methodë¡œ exampleë“¤ê³¼ inputì„ ëª¨ë‘ latent vectorë¡œ ë³€í™˜í•œ í›„ ì´ vectorë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, train example setê³¼ test example setì„ ë‚˜ëˆ„ê³  train example setë§Œ ê°€ì§€ê³  retrieverë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. í•™ìŠµì„ ìœ„í•´ ë¨¼ì € í•™ìŠµë°ì´í„°ë¥¼ ëª¨ë‘ latent vectorë¡œ ë³€í™˜í•œ í›„ GPT-Neoì™€ ê°™ì€ ì‘ì€ ëª¨ë¸ë¡œ training dataì˜ ì •ë‹µ labelì„ í†µí•´ í•œ exampleê³¼ ë‹¤ë¥¸ example ì‚¬ì´ì˜ socreë¥¼ ì–»ìŠµë‹ˆë‹¤. ì´ scoreë¥¼ í†µí•´ top-k, bottom-k sampleì„ ì–»ê³ , input dataë¥¼ ì¸ì½”ë”© í•˜ëŠ” $E_X$ì™€ demonstrationì„ ì¸ì½”ë”© í•˜ëŠ” $E_P$ë¥¼ contrastive learningì„ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.

![image.png](/assets/images/in_posts/a-survey-on-icl/2e751cd2-0102-4b26-99c8-b45f6fa09c35.png)

[CEIL](#ref11)ì€ ìµœì ì˜ demonstrationì„ example ê°ê° í•˜ë‚˜ì”© ê³„ì‚°í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë¼ subset selection ë¬¸ì œë¡œ ìƒì •í•˜ê³  í•œë²ˆì— ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. DPPë¥¼ ì´ìš©í•´ ê³ ì •ëœ exampleì˜ ì§‘í•©ë‚´ì—ì„œ ë¶€ë¶„ì§‘í•©ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•˜ì—¬ ë¶€ë¶„ì§‘í•© ìì²´ì— ëŒ€í•œ contrastive learningì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” DPPë¥¼ ì•½ê°„ ë³€í˜•í•˜ì—¬ test setê³¼ ê´€ë ¨ì´ ë†’ìœ¼ë©´ì„œë„ demonstration ë¶€ë¶„ì§‘í•© ë‚´ë¶€ì˜ exampleë¼ë¦¬ ë‹¤ì–‘ì„±ì„ ë³´ì¥í•˜ëŠ” retrieverë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

#### Demonstation Reformatting

Demonstration reformattingì€ LLMì„ ì‚¬ìš©í•´ì„œ demonstrationì„ ì¬êµ¬ì„±í•˜ê±°ë‚˜ ìƒˆë¡œ ì‘ì„±í•˜ëŠ” ë°©ë²•ì„ ë§í•©ë‹ˆë‹¤.

![image.png](/assets/images/in_posts/a-survey-on-icl/9f5e26ee-e216-4050-9ce1-11951c8a15da.png)

[SG-ICL](#ref12)ì€ ICLì˜ demonstration ì˜ì¡´ì„±ì„ ì œê±°í•˜ê¸° ìœ„í•´ LLMì„ í†µí•´ ìƒˆë¡œìš´ demonstrationì„ ìƒì„±í•œ í›„ì— ìƒì„±ëœ demonstrationìœ¼ë¡œ inferenceë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

![image.png](/assets/images/in_posts/a-survey-on-icl/image%201.png)

[ICV(In-context vectors)](#ref13)ëŠ” demonstration textë¥¼ ë¯¸ë¦¬ LLMì— ì…ë ¥í•˜ì—¬ ë§ˆì§€ë§‰ tokenì— ëŒ€í•œ latent vector(ICV)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. Inferenceì‹œì—ëŠ” ICVë¥¼ ê° ë ˆì´ì–´ì˜ latent vectorì— ë”í•´ì¤Œìœ¼ë¡œì¨ in-context learningì˜ context lengthì— ëŒ€í•œ ì œí•œì„ ê·¹ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Scoring Function

Scoring functionì€ LLMì˜ ì˜ˆì¸¡ì„ íŠ¹ì • answerì˜ likelihoodë¡œ í‰ê°€í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. LLMì˜ ì¶œë ¥ì€ token sequenceë¡œ, ìƒì„±ë  ìˆ˜ ìˆëŠ” ë‹µë³€ì˜ ìˆ˜ê°€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë§ì€ NLP taskì—ì„œëŠ” ì •í™•í•œ ì •ë‹µ(golden label, specific class ë“±ë“±)ì„ ìš”êµ¬í•©ë‹ˆë‹¤. Scoring functionì€ ìƒì„±ëœ í† í° ì‹œí€€ìŠ¤ì—ì„œ ì •ë‹µì„ ì¶”ì¶œí•˜ê³ , ê·¸ ë‹µì´ ì˜¬ë°”ë¥¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë³„í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ê°€ì¥ ì§ê´€ì ìœ¼ë¡œëŠ” LLMì´ ì¶œë ¥í•  ìˆ˜ ìˆëŠ” ëª¨ë“  token ì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” tokenì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ LLMì´ ì…ë ¥ ë’¤ì— ë°”ë¡œ ì •ë‹µì„ ì¶œë ¥í•˜ë„ë¡ í•´ì•¼ í•˜ë¯€ë¡œ templateë¥¼ ë§Œë“œëŠ”ë° ë§ì€ ì œì•½ì´ ë”°ë¦…ë‹ˆë‹¤.

ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” Perplexity(PPL)ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. PPLì€ ì •ë‹µ tokenì˜ í¬ì§€ì…˜ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ ì •í™•í•œ labelì„ ì¶”ì¶œí•˜ëŠ”ë° ì¶”ê°€ì ì¸ ê³„ì‚° ë¹„ìš©ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.

![image.png](/assets/images/in_posts/a-survey-on-icl/image%202.png)

[Channel models](#ref14) ë°©ë²•ì€ text classification ë¬¸ì œì— í•œí•´ì„œ,
input $x$ ê°€ ì£¼ì–´ì¡Œì„ë•Œ output $y$ì˜ í™•ë¥  ê³„ì‚°ì‹œ $P(y|x)$ë¡œ ê³„ì‚°í•˜ëŠ” Direct model ë°©ì‹ê³¼ ë‹¬ë¦¬ output $y$ê°€ ì£¼ì–´ì¡Œì„ë•Œ inputì´ $x$ì¼ í™•ë¥  $P(x|y)P(y)$ì„ êµ¬í•˜ëŠ” channel modelì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ì •ë³´ì´ë¡ ì˜ noisy modelì´ ë“±ì¥í•œ ë™ê¸°ì™€ ë¹„ìŠ·í•˜ê²Œ modelì´ ë¶ˆê· í˜•í•œ ë°ì´í„°ë‚˜ ì ì€ ë¼ë²¨ë§ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆì„ë•Œ ë³´ë‹¤ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” prompting ë°©ì‹ê³¼ fine-tuningë°©ì‹ ëª¨ë‘ ì ìš©í•´ì„œ ë¹„êµë¥¼ í–ˆìŠµë‹ˆë‹¤.

## Finally

ì´ ë‹¤ìŒ ì„¹ì…˜ë¶€í„°ëŠ” ICLì˜ ê·¼ë³¸ì ì¸ ì‘ë™ì›ë¦¬ë¥¼ íŒŒí•´ì¹˜ê³ ì í•˜ëŠ” ë¶„ì„ ë…¼ë¬¸ë“¤ì´ ë‚˜ì˜µë‹ˆë‹¤. í•˜ì§€ë§Œ ì €ëŠ” ë¶„ì„ë…¼ë¬¸ë“¤ ê¹Œì§€ëŠ” ë‹¤ ì½ì–´ë³´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ ğŸ¥²Â ë¶„ì„ì€ ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ë¶„ì•¼ì´ê¸´ í•˜ì§€ë§Œ ì‘ìš©ê³¼ ë¶„ì„ì€ ì¡°ê¸ˆ ê²°ì´ ë‹¬ë¼ì„œ ì €ëŠ” ì­‰ì­‰ ì½ê¸°ê°€ í˜ë“¤ë”ë¼ê³ ìš” ğŸ˜­ ê¸°ì¡´ì˜ language modelë“¤ì„ ë¶„ì„í•˜ë˜ ë¶„ë“¤ì´ ICLì˜ ë¶„ì„ë„ ì—°êµ¬í•˜ì‹œëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ìª½ë„ ì¢‹ì€ ë…¼ë¬¸ë“¤ì´ ë§ì´ ë‚˜ì˜¤ê³  ìˆê³  ICLì˜ ê¸°ë³¸ ì›ë¦¬ì— ëŒ€í•´ì„œ ë‹¤ì‹œí•œë²ˆ ìƒê°í•´ë³´ê±°ë‚˜ ì°¸ì‹ í•œ ë°©í–¥ìœ¼ë¡œ ìƒê°í•´ë³¼ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì‹œê°„ì´ ë˜ëŠ”ëŒ€ë¡œ ì½ê³ ì‹¶ë„¤ìš”.

ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œ ì™„ì„±ë„ê°€ ë†’ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ê¹”ë”í•œ ë…¼ë¬¸ë„ ë§ì•˜ì§€ë§Œ arxivì—ë§Œ ì˜¬ë¼ì™€ìˆëŠ” ë…¼ë¬¸ë“¤ ì¤‘ì—ëŠ” ì•„ì§ ë…¼ë¦¬ë‚˜ ê·¼ê±°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ê¸€ì´ ì—°ê²°ì´ ë˜ì§€ ì•ŠëŠ”ê²½ìš°ë„ ìˆì—ˆìŠµë‹ˆë‹¤ ğŸ˜…Â ë‹¤ë¥´ê²Œ ìƒê°í•´ë³´ë©´ ICLì´ë¼ëŠ” ì£¼ì œê°€ ê·¸ë§Œí¼ ëœ¨ê±°ìš´ ê°ìë¼ëŠ” ì‚¬ì‹¤ì€ ë³€í•¨ì´ ì—†ëŠ”ê²ƒ ê°™êµ°ìš”! ì—´ì‹¬íˆ ì—°êµ¬í•´ì„œ ì €ì™€ ì—¬ëŸ¬ë¶„ë“¤ ëª¨ë‘ ì¢‹ì€ ë…¼ë¬¸ì„ ë‚¼ ìˆ˜ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ¤—

ì˜ëª»ëœ ë¶€ë¶„ì— ëŒ€í•œ ì§€ì ì€ ì–¸ì œë‚˜ í™˜ì˜ì…ë‹ˆë‹¤.

## ì°¸ê³ ë¬¸í—Œ

<span id="ref1">[1] [A Survey on In-context Learning](https://arxiv.org/abs/2301.00234) (Dong et al., arxiv 2022)</span>

<span id="ref2">[2] [In-context Pretraining: Language Modeling Beyond Document Boundaries](https://arxiv.org/abs/2310.10638) (Shi et al., arxiv 2023)</span>

<span id="ref3">[3] [https://www.harmdevries.com/post/context-length/](https://www.harmdevries.com/post/context-length/)</span>

<span id="ref4">[4] [MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning](https://arxiv.org/abs/2403.06914) (Li et al., ICLR 2024)</span>

<span id="ref5">[5] [MetaICL: Learning to Learn In Context](https://aclanthology.org/2022.naacl-main.201)Â (Min et al., NAACL 2022)</span>

<span id="ref6">[6] [Improving In-Context Few-Shot Learning via Self-Supervised Training](https://aclanthology.org/2022.naacl-main.260)Â (Chen et al., NAACL 2022)</span>

<span id="ref7">[7] [What Makes Good In-Context Examples for GPT-3?](https://aclanthology.org/2022.deelio-1.10)Â (Liu et al., DeeLIO 2022)</span>

<span id="ref8">[8] [An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels](https://aclanthology.org/2022.acl-long.60)Â (Sorensen et al., ACL 2022)</span>

<span id="ref9">[9] [Learning To Retrieve Prompts for In-Context Learning](https://aclanthology.org/2022.naacl-main.191)Â (Rubin et al., NAACL 2022)</span>

<span id="ref10">[10] [Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering](https://aclanthology.org/2023.acl-long.79)Â (Wu et al., ACL 2023)</span>

<span id="ref11">[11] [Compositional exemplars for in-context learning](https://dl.acm.org/doi/10.5555/3618408.3620070)  (Ye et al., ICML'23)</span>

<span id="ref12">[12] [Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator](https://arxiv.org/abs/2206.08082) (Kim et al., arxiv 2022</span>

<span id="ref13">[13] [In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering](https://arxiv.org/abs/2311.06668) (Liu et al., arxiv 2023)</span>

<span id="ref14">[14] [Noisy Channel Language Model Prompting for Few-Shot Text Classification](https://aclanthology.org/2022.acl-long.365)Â (Min et al., ACL 2022)</span>